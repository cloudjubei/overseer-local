{
  "id": "7e07f377-262d-4ffc-a702-fa20ef521315",
  "status": "+",
  "title": "Agentic chat interface",
  "description": "The electron app developed in `src/` needs extra features. References `README.md`, `docs/FILE_ORGANISATION.md`. It has the Tasks section (task 2), the documents section (task 3) and we need to have a Chat (this task). Allow communication with an LLM agent (configurable) to discuss the project, the tasks, the documents and any other project related things. All in a chat UI standard, that allows mentioning project docs, uploading new ones etc.",
  "features": [
    {
      "id": "62121502-adc3-4d23-8cb9-f440010a8153",
      "status": "+",
      "title": "Add Chat route and basic UI skeleton",
      "description": "Update the renderer (e.g., main app.js or index.js) to handle window.location.hash === '#chat' by rendering a new ChatView component (in src/renderer/chatView.js). The ChatView should include a header 'Project Chat', a div for message list, a textarea for input, and a send button. Ensure consistent styling and navigation with existing views like tasksListView and docsBrowserView. Add a link or tab in the main UI to navigate to #chat.",
      "plan": "",
      "context": [
        "src/renderer/screens/ChatView.tsx"
      ],
      "acceptance": []
    },
    {
      "id": "fdd45a08-3010-4b1a-8457-f1bc52bb4319",
      "status": "+",
      "title": "Manage chat state and render messages",
      "description": "In ChatView, use React state (or equivalent) for a messages array of objects {role: 'user'|'assistant', content: string}. Render the messages in the message div, with user messages on the right, assistant on the left, styled as chat bubbles. Handle empty state with a message like 'Start chatting about the project'.",
      "plan": "",
      "context": [
        "src/renderer/screens/ChatView.tsx"
      ],
      "acceptance": []
    },
    {
      "id": "874d74fe-a402-4ca4-8a73-b6fb9ea849d9",
      "status": "+",
      "title": "Implement message input and dummy response logic",
      "description": "In ChatView, add event handler for send button click or Enter key in textarea. Get input value, append {role: 'user', content} to messages state, clear input, then append a dummy {role: 'assistant', content: 'Echo: ' + content} to simulate response. Scroll to bottom after updates.",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "a56509a7-ba64-49fd-99f5-022a9f6ceef5",
      "status": "+",
      "title": "Add LLM configuration UI and local storage - separate class to manage this",
      "description": "In ChatView, add a settings button that opens a form (modal or section) for LLM config: apiBaseUrl (default 'https://api.openai.com/v1'), apiKey, model (default 'gpt-4o'). Save values to localStorage on submit. Load and apply on component mount. Show warning if not configured.",
      "plan": "",
      "context": [
        "src/main.js",
        "src/preload.js",
        "src/docs/indexer.js",
        "src/tasks/indexer.js",
        "src/renderer/screens/ChatView.tsx"
      ],
      "acceptance": []
    },
    {
      "id": "77bdff96-024d-41e9-97a9-479588e9aedc",
      "status": "+",
      "title": "Add IPC handler for basic non-streaming LLM completion",
      "description": "In main process, install 'openai' package if needed. Add ipcMain.handle('chat:completion', async (event, {messages, config}) => { const openai = new OpenAI({baseURL: config.apiBaseUrl, apiKey: config.apiKey}); const systemPrompt = {role: 'system', content: 'You are a helpful project assistant. Discuss tasks, documents, and related topics.'}; const response = await openai.chat.completions.create({model: config.model, messages: [systemPrompt, ...messages], stream: false}); return response.choices[0].message; }). Handle errors gracefully.",
      "plan": "",
      "context": [
        "src/main.js",
        "src/preload.js",
        "src/docs/indexer.js",
        "src/tasks/indexer.js",
        "src/renderer/screens/ChatView.tsx"
      ],
      "acceptance": []
    },
    {
      "id": "c127465a-2a0f-4a61-86fc-3c699103a7ae",
      "status": "+",
      "title": "Integrate basic LLM call in ChatView",
      "description": "Update send handler in ChatView: Allow a user to send a message. Show loading indicator while waiting. Handle errors with a message.",
      "plan": "",
      "context": [
        "src/main.js",
        "src/preload.js",
        "src/renderer/screens/ChatView.tsx"
      ],
      "acceptance": []
    },
    {
      "id": "1f09ca05-7dba-441b-a720-06f35478c1c8",
      "status": "+",
      "title": "Extend IPC for tool calling in agent loop",
      "description": "Update 'chat:completion' handler to support tools. Define an empty tools array for now. In the handler, implement a loop: call completions.create with tools, if response has tool_calls, execute each (call function and get result), append {role: 'tool', content: result, tool_call_id} to messages, repeat until no tool_calls, then return the final content message.",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "b164f116-bb80-46a3-be0e-008e6204ad8c",
      "status": "+",
      "title": "Add project read tools for agent",
      "description": "In the tools array of the completion handler, add: 1. list_tasks (no params, returns JSON.stringify(taskIndexer.getIndex().tasksById)), 2. list_docs (no params, returns JSON.stringify(docsIndexer.getIndex().docsTree)), 3. read_doc (param: path, returns fs.readFileSync(docsIndexer.getIndex().docsDir + '/' + path, 'utf8') or error). Update system prompt: 'Use tools to query project info. If user mentions @path, use read_doc.'",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "e5d333bf-5051-46b3-a52b-8599733720c2",
      "status": "+",
      "title": "Add create_doc tool for agent",
      "description": "Add to tools: create_doc (params: name, content)",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "3b4b4a78-0904-4dde-90bc-c9be16ae80b9",
      "status": "+",
      "title": "Add autocomplete for mentioning docs",
      "description": "In ChatView input textarea, detect typing starting with '@', show a dropdown autocomplete list of matching paths from docsIndex. On select, insert the full '@path' into the input. Subscribe to docs updates to refresh list.",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "12de1129-006b-440c-a178-a2672f67e5ce",
      "status": "+",
      "title": "Add file upload UI and IPC for docs",
      "description": "In ChatView, add an attach button next to input, with hidden file input.",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "37558f81-cc33-4249-9423-91e55ca6ff41",
      "status": "+",
      "title": "Integrate upload into chat flow",
      "description": "After successful upload via IPC, append a user message to state: 'Uploaded document to @' + returnedPath. This allows mentioning it immediately and triggers LLM awareness via tools.",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "f5adab71-5f64-4303-8c57-b4f78760d867",
      "status": "+",
      "title": "Add chat history persistence",
      "description": "",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "7951b9cf-c483-4a02-9717-ca3742d62b00",
      "status": "+",
      "title": "Allow viewing chat histories and resume an old conversation",
      "description": "",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "9fa712c1-649f-433a-a5f5-bfa482e3b6db",
      "status": "+",
      "title": "LLM connector abstraction and providers (LiteLLM, OpenAI-compatible)",
      "description": "Design an abstraction for chat providers. Implement connectors for: (1) LiteLLM-compatible HTTP endpoint, (2) OpenAI-compatible custom base URL to support LM Studio / Ollama. Configurable API key, base URL, model name, timeouts.",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "14edaedd-8dfe-4047-965a-3130ec458140",
      "status": "+",
      "title": "Settings screen is more advanced",
      "description": "The user should be able to configure various LLMs using various providers - this requires a more complex screen that should be accessible via Chat. The user needs to select a model from some common models (OpenAI, Anthropic, xai/grok, google/gemini) and based on the interface (LiteLLM) it works out API URL + provides a dropdown to available models (the user can also go custom and type it in themselves).",
      "plan": "",
      "context": [],
      "acceptance": [],
      "dependencies": []
    },
    {
      "id": "c3b28472-ea04-4b83-8048-3e389cbe8f12",
      "status": "+",
      "title": "Switching models",
      "description": "User can switch between different models that were configured directly in the chat. There's a dropdown with all the options that were previously configured and the user can choose which chat to use. If there was a switch in a conversation it is shown which LLM replied.",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "0dfdfd09-d878-4eb9-a5af-b8ca5326bd08",
      "status": "+",
      "title": "Custom local provider - LM Studio",
      "description": "Allow using LM Studio's API server that lists models at http://localhost:1234/v1/models - there should be a check to see that it's running and then all the available models can be listed. To post messages to: http://localhost:1234/v1/chat/completions",
      "plan": "",
      "context": [],
      "acceptance": []
    },
    {
      "id": "1612a80e-2948-477e-8bf4-dc7ffa5bbc36",
      "status": "+",
      "title": "Settings needs adapting",
      "description": "We only accept the user specifying an LLM provider by passing in the api url, the api key and the model name. We want to have some presets available - the user can select a common provider (openai, gemini, anthropic, xai, local) and then the api url should get populated for the user. The list of models for local can be retrieved via the standard provider but for the rest we can include the latest models for that provider.",
      "plan": "",
      "context": [
        "src/renderer/navigation/ModalHost.tsx",
        "src/renderer/navigation/Navigator.tsx",
        "src/renderer/screens/TasksView.tsx",
        "src/renderer/tasks/TaskDetailsView.tsx"
      ],
      "acceptance": []
    },
    {
      "id": "815c3f72-9c23-44e8-b8f3-0dea0e869fe0",
      "status": "+",
      "title": "Settings for LLMs fixes",
      "description": "When a LLM config is saved it doesn't update the list of configs in settings. The dropdown inside the LLMConfig popup doesn't show when clicked.",
      "plan": "",
      "context": [
        "src/renderer/screens/SettingsView.tsx",
        "src/renderer/settings/SettingsLLMConfigModal.tsx",
        "src/renderer/navigation/ModalHost.tsx",
        "src/renderer/navigation/Navigator.tsx"
      ],
      "acceptance": []
    }
  ],
  "featureIdToDisplayIndex": {
    "62121502-adc3-4d23-8cb9-f440010a8153": 0,
    "fdd45a08-3010-4b1a-8457-f1bc52bb4319": 1,
    "874d74fe-a402-4ca4-8a73-b6fb9ea849d9": 2,
    "a56509a7-ba64-49fd-99f5-022a9f6ceef5": 3,
    "77bdff96-024d-41e9-97a9-479588e9aedc": 4,
    "c127465a-2a0f-4a61-86fc-3c699103a7ae": 5,
    "1f09ca05-7dba-441b-a720-06f35478c1c8": 6,
    "b164f116-bb80-46a3-be0e-008e6204ad8c": 7,
    "e5d333bf-5051-46b3-a52b-8599733720c2": 8,
    "3b4b4a78-0904-4dde-90bc-c9be16ae80b9": 9,
    "12de1129-006b-440c-a178-a2672f67e5ce": 10,
    "37558f81-cc33-4249-9423-91e55ca6ff41": 11,
    "f5adab71-5f64-4303-8c57-b4f78760d867": 12,
    "7951b9cf-c483-4a02-9717-ca3742d62b00": 13,
    "9fa712c1-649f-433a-a5f5-bfa482e3b6db": 14,
    "14edaedd-8dfe-4047-965a-3130ec458140": 15,
    "c3b28472-ea04-4b83-8048-3e389cbe8f12": 16,
    "0dfdfd09-d878-4eb9-a5af-b8ca5326bd08": 17,
    "1612a80e-2948-477e-8bf4-dc7ffa5bbc36": 18,
    "815c3f72-9c23-44e8-b8f3-0dea0e869fe0": 19
  }
}